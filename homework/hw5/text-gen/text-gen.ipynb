{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "709c5453",
   "metadata": {},
   "source": [
    "## <font color='blue'>Text generation using an n-gram model</font>\n",
    "\n",
    "In this problem, we'll train a language model on Shakespeare and use it to generate text. Our model will be bare-bones but has the same underlying ideas as much more advanced models that are in wide use. Simply put, language models assign a likelihood (probability) to a given sentence. For example, the sentence \"dog is barking\" has a higher likelihood than \"bark dog is\", which is grammatically incoherent. \n",
    "\n",
    "### <font color='blue'>Bigram and trigram models</font>\n",
    "\n",
    "Suppose we have a sentence composed of words (or tokens) $x_1, x_2, ..., x_n$. We want to be able to compute the probability of this sequence, $P(x_1, x_2, ... x_n)$. The distribution can always be factored as\n",
    "\n",
    "$$  P(x_1, x_2, ... x_n)  =  P(x_1) \\prod_{i=2}^{n}P(x_i|x_{i-1}, x_{i-2}, ..., x_1) .$$\n",
    "\n",
    "However, these conditional probabilities are hard to model for even medium-sized vocabularies. One simplification is to make a <em>first-order Markov assumption</em>, which says that the probability of a word given all previous words is equal to the probability of the word given just the one word before it, that is,\n",
    "\n",
    "$$ P(x_i | x_1, x_2, ..., x_{i-1}) = P(x_i | x_{i-1}). $$\n",
    "\n",
    "This allows us to factor\n",
    "\n",
    "$$ P(x_1, x_2, ... x_n)  = \\prod_{i=1}^{n}P(x_i|x_{i-1}) $$\n",
    "\n",
    "where we take $x_0$ to be a special \"START\" symbol.\n",
    "\n",
    "The above formulation is called a <em>bigram</em> language model, since it is based on pairs of consecutive words. If we expand the context to include the two previous words, we get a <em>trigram</em> model,\n",
    "    \n",
    "$$ P(x_1, x_2, ... x_n)  = \\prod_{i=1}^{n}P(x_i|x_{i-2}, x_{i-1}) $$\n",
    "\n",
    "where $x_{-1}, x_0$ are special start symbols. \n",
    "\n",
    "In the same way, we can define an <em>n-gram</em> model by expanding the context to include the $n-1$ previous words.\n",
    "\n",
    "### <font color='blue'>Learning an $n$-gram model</font>\n",
    "\n",
    "In this problem, we will use <em>maximum-likelihood estimation</em> to learn an $n$-gram model. For this, we simply need to count the number of occurrences of each $n$-gram and $n-1$ gram in the corpus.\n",
    "\n",
    "For instance, to train a trigram model, let $c(u,v,w)$ be the number of times that trigram $(u,v,w)$ is seen in the training corpus and let $c(u,v)$ be the number of times that bigram $(u,v)$ is seen. The maximum likelihood estimate of $P(w|u,v)$ is then \n",
    "$$ P(w|u,v) = \\frac{c(u,v,w)}{c(u,v)} .$$\n",
    "Of course, we might want to smooth this using Laplace smoothing or something similar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82556bde",
   "metadata": {},
   "source": [
    "### <font color='blue'>Reading in the data</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47320ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33029d64",
   "metadata": {},
   "source": [
    "The provided text file `shakespeare.txt` contains a selection of Shakepeare's sonnets and plays. It also contains stage directions, copyright notices, and various other things that we won't bother removing. You should take a quick look at this file. We will read in a list of all sentences in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b3fff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "with open(\"shakespeare.txt\", \"r\", encoding=\"utf8\") as f:\n",
    "    text = f.read()\n",
    "    text = text.split(\".\")\n",
    "    for sentence in text:\n",
    "        sentence += \".\"\n",
    "        sentences.append(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4761bba8",
   "metadata": {},
   "source": [
    "Here is an example of one of the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dc42cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[200]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d429398c",
   "metadata": {},
   "source": [
    "Let's set aside the very first sentence (we'll use it later). We'll extract all words from the remainder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07886413",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_play = sentences[0]\n",
    "sentences = sentences[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81bae7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set()\n",
    "for sent in sentences:\n",
    "    for word in sent.split():\n",
    "        words.add(word)\n",
    "vocab = list(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfb53f3",
   "metadata": {},
   "source": [
    "### <font color='blue'>Class: NGramModel</font> \n",
    "\n",
    "Next, we define a class that will hold an n-gram model. Go through the methods of this class and make sure you understand what is happening in each method as you'll be asked to fill in some incomplete methods. Also take note of the data structures used in various methods.\n",
    "\n",
    "<font color='magenta'>Part (a): Complete the `generate_n_grams` method. Given a sequence (list) of tokens, you need to return all of its ngrams. For example, for `tokens = ['Unsupervised',  'Learning', 'is', 'fun', '.']`, the trigrams would be `[(['START', 'START'], 'Unsupervised'), (['START', 'Unsupervised'], 'Learning') ...]`.</font>\n",
    "\n",
    "<font color='magenta'>Part (b): Complete the `get_prob` method which takes in a list of context tokens and a target token and returns the probability of the target given the context. You will need to use the data structures that get built in the `fit` method.</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb239f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple\n",
    "\n",
    "class NGramModel(object):\n",
    "    def __init__(self, n: int) -> None:\n",
    "        self.n = n\n",
    "        self.n_grams = dict() # Stores unique n-grams\n",
    "        self.context_count = dict() # Stores count of contexts\n",
    "        self.ngram_count = dict() # Stores count of ngrams: (context, token)\n",
    "\n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        # Tokenize the given sentence 'text'\n",
    "        # Treat punctuation as a separate token.\n",
    "        # Add space before punctuation.\n",
    "        # Split using spaces.\n",
    "\n",
    "        for ch in string.punctuation:\n",
    "            text = text.replace(ch, \" \" + ch)\n",
    "        tokens = text.strip().split()\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def generate_n_grams(self, tokens: List[str]) -> List[Tuple[List[str], str]]:\n",
    "        # Generate all n-grams from the given list of tokens\n",
    "        # Insert (n-1) <START> tokens before each sentence\n",
    "        tokens = (self.n-1)*[\"<START>\"] + tokens\n",
    "        n_grams = list()\n",
    "\n",
    "        \"\"\"\n",
    "        n_grams is a list where each element is\n",
    "        ([n-1 context tokens], token)\n",
    "        \"\"\"\n",
    "        #raise NotImplementedError\n",
    "        # Q1:  Complete the generate_n_grams method and make sure that your return type matches\n",
    "        #      the expected return type.\n",
    "        ## SOLUTION:\n",
    "        \n",
    "\n",
    "        return n_grams\n",
    "\n",
    "    def fit(self, text: str) -> None:\n",
    "        # Takes a sentence 'text'\n",
    "        # Generates all n-grams in the sentence\n",
    "        # Then updates counts\n",
    "        new_n_grams = self.generate_n_grams(self.tokenize(text))\n",
    "        for context, target in new_n_grams:\n",
    "            # Add context to context dict and store count \n",
    "            if tuple(context) in self.context_count:\n",
    "                self.context_count[tuple(context)] += 1.0\n",
    "            else:\n",
    "                self.context_count[tuple(context)] = 1.0\n",
    "                \n",
    "            # Save unique n_grams.\n",
    "            # This part is used for generation only.\n",
    "            if tuple(context) in self.n_grams:\n",
    "                if target not in self.n_grams[tuple(context)]:\n",
    "                    self.n_grams[tuple(context)].append(target)\n",
    "            else:\n",
    "                self.n_grams[tuple(context)] = [target]\n",
    "\n",
    "            # Store n_gram counts\n",
    "            new_n_gram = (tuple(context), target)\n",
    "            if new_n_gram in self.ngram_count:\n",
    "                self.ngram_count[new_n_gram] += 1.0\n",
    "            else:\n",
    "                self.ngram_count[new_n_gram] = 1.0\n",
    "                \n",
    "            \n",
    "\n",
    "    def get_prob(self, context: List[str], target: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the probability of the target token\n",
    "        given the context.\n",
    "        \"\"\"\n",
    "        # raise NotImplementedError\n",
    "        # Q2: Complete the get_prob method to calculate the probabilites of input ngrams.\n",
    "        ## SOLUTION:\n",
    "        \n",
    "        \n",
    "    \n",
    "    def predict_token(self, context: List[str]) -> str:\n",
    "        \"\"\"\n",
    "        Predict the next token given context.\n",
    "        A slight randomness ensures we generate a diverse token\n",
    "        with the same context.\n",
    "        \"\"\"\n",
    "        r = random.random()\n",
    "        # store the probability of each token.\n",
    "        token_probs = dict()\n",
    "        \n",
    "        try:\n",
    "            tokens_of_interest = self.n_grams[tuple(context)]\n",
    "            for token in tokens_of_interest:\n",
    "                token_probs[token] = self.get_prob(context, token)\n",
    "        except KeyError: # similar to Laplace smoothing; returns a random word from the vocab.\n",
    "            ridx = random.randint(0, len(words))\n",
    "            return vocab[ridx]\n",
    "            \n",
    "        \n",
    "        sum = 0.0\n",
    "        for key in sorted(token_probs):\n",
    "            sum += token_probs[key]\n",
    "            # When the probability sum is > random number\n",
    "            # return the current token.\n",
    "            if sum > r:\n",
    "                return key\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a226b5e9",
   "metadata": {},
   "source": [
    "### <font color='blue'>Routines for fitting data and generating text</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30c2868",
   "metadata": {},
   "source": [
    "`create_and_fit_model` defines an n-gram model and fits it to data. Its parameters are `n` (the order of the model) and `sentences` (collection of sentences).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a183de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_fit_model(n, sentences):\n",
    "    \"\"\"\n",
    "    This is the key function that defines and fits an n-gram model.\n",
    "    It takes in n and a list of sentences.\n",
    "    It creates an n-gram model and then calls the `fit` method on \n",
    "    one sentence at a time to generate counts.\n",
    "    \"\"\"\n",
    "    model = NGramModel(n)\n",
    "    for sent in sentences:\n",
    "        model.fit(sent)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8a5bb0",
   "metadata": {},
   "source": [
    "`generate_text` uses an n-gram model to generate text, starting from a prompt (which might be empty). It takes as input the `model`, the number of words to generate (`n_outs`) and the `prompt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b51c8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model: NGramModel, n_outs: int, prompt=None) -> str:\n",
    "    \"\"\"\n",
    "    Generates n_outs words using the trained\n",
    "    ngram model.\n",
    "    \"\"\"\n",
    "    n = model.n\n",
    "    # All sentence are initialized with the <START> token\n",
    "    \n",
    "    if prompt is not None:\n",
    "        prompt_tokens = model.tokenize(prompt)\n",
    "        context_queue = prompt_tokens[-(n-1):]\n",
    "    else:\n",
    "        context_queue = (n-1) * [\"<START>\"]\n",
    "    result = list()\n",
    "\n",
    "    for _ in range(n_outs):\n",
    "        pred_token = model.predict_token(context_queue)\n",
    "        result.append(pred_token)\n",
    "\n",
    "        context_queue.pop(0)\n",
    "\n",
    "        if pred_token == \".\":\n",
    "            # If sentence done. Start a new sentence.\n",
    "            context_queue = (n-1) * [\"<START>\"]\n",
    "        else:\n",
    "            context_queue.append(pred_token)\n",
    "\n",
    "    return \" \".join(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc401f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_generated_text(model):\n",
    "    \"\"\"\n",
    "    Prints a 100-word blurb from the provided model.\n",
    "    \"\"\"\n",
    "    num_gen_words = 100\n",
    "    print(f'{\"=\"*50}\\nGenerated text:')\n",
    "    print(\"\\n\")\n",
    "    print(generate_text(model, num_gen_words))\n",
    "    print(f'{\"=\"*50}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb221db",
   "metadata": {},
   "source": [
    "### <font color='blue'>Experimenting with text generation</font>\n",
    "\n",
    "<font color='magenta'>Part (c): Use `create_and_fit` to create n-gram models for n=2 and n=3 and fit them to the provided text from Shakespeare. Then use `print_generated_text` to generate text from each of the two models. Report the two resulting texts.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbece21",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SOLUTION:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6cf7ec",
   "metadata": {},
   "source": [
    "### <font color='blue'>Computing likelihoods</font>\n",
    "\n",
    "In the part, we'll calculate the log-likelihood of a sentence using the models that we just created. This will give us a concrete indication of why incorporating more context is helpful.\n",
    "For a given sentence $s$ with $n$ tokens $(x_1, \\ldots, x_n)$, the log-likelihood is defined as\n",
    "$$ L(s) = \\sum_{i=1}^{n}\\log p(x_i|\\mbox{context}) $$\n",
    "For a unigram model, this maximum-likelihood estimates of $p(x_i)$ would simply be\n",
    "$$ p(x_i) = \\frac{c(x_i)}{N}$$ \n",
    "where N is the total number of words in the dataset and $c(x_i)$ is the number of occurrences of $x_i$.  \n",
    "Similarly, for a bigram model, we have\n",
    "$$ p(x_i|\\mbox{context}) = \\frac{c(x_{i-1}, x_i)}{c(x_{i-1})} .$$\n",
    "In our code, these probabilities are provided by `get_prob` in `NGramModel`.\n",
    "\n",
    "<font color='magenta'>Part (d): Write a function that takes an `NGramModel` and a sentence and returns the log-likelihood of that sentence. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e5d9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_ll(model, text):\n",
    "    \"\"\"\n",
    "    Takes in an n-gram model and a sample text.\n",
    "    Returns the log-likelihood of the text under the given model.\n",
    "    \"\"\"\n",
    "    import math\n",
    "    ll = 0\n",
    "    # raise NotImplementedError\n",
    "    ## SOLUTION:\n",
    "    # 1. Tokenize the text.\n",
    "\n",
    "    \n",
    "    # 2. Generate ngrams for the tokens.\n",
    "\n",
    "    \n",
    "    # 3. Loop through the ngrams, calculate log prob for each ngram and update ll.\n",
    "\n",
    "    \n",
    "    return ll"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa914c65",
   "metadata": {},
   "source": [
    "<font color='magenta'>Q4 (cont'd). Use the sentence \"And on a love-book pray for my success?\" as your input. Calculate the log-likelihood of this sentence under a unigram model and a bigram model and report those numbers.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0fc28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"And on a love-book pray for my success?\"\n",
    "## SOLUTION:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1804fd",
   "metadata": {},
   "source": [
    "### <font color='blue'>Text completion</font>\n",
    "\n",
    "<font color='magenta'>Part (e): Given a prompt from `test_play` (the held out part of the first sonnet), generate 20 words of text (using the function `generate_text`) from the bigram and the trigram model.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f9e6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION\n",
    "print(test_play)\n",
    "prompt = \"From fairest creatures we desire increase\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c94d655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "generate_text(trigram_model, 20, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d228ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "generate_text(bigram_model, 20, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09317587",
   "metadata": {},
   "source": [
    "There are various tweaks that would improve this model: for instance, careful <em>smoothing</em> and using longer-range dependencies (via variable-length Markov models such as <em>probabilistic suffix trees</em>). The next big boost in performance would come from replacing tabular estimates of conditional probabilities $P(x_i|x_1, ...., x_{i-1})$ by <em>recurrent neural nets</em>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "189px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
