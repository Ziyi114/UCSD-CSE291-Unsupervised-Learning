{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Programming Assignment: Image Representations</font>\n",
    "\n",
    "In this problem, we will study how different representations of images can affect the performance of a nearest neighbor classifier.\n",
    "\n",
    "We will experiment with CIFAR-10 data set, which has 50,000 training images and 10,000 test images, with ten different classes (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck). The images are in color, of size 32 × 32.\n",
    "\n",
    "We will investigate the following representions:\n",
    "1. Raw pixel space\n",
    "2. Histogram-of-gradients (HOG) features\n",
    "3. Convolutional Neural Network (CNN) features\n",
    "    * Intermediate-level and high-level features extracted from a **pretrained** CNN \n",
    "    * Intermediate-level and high-level features extracted from a **randomly initialized** CNN \n",
    "\n",
    "#### Instructions\n",
    "1. Install the required libraries\n",
    "    * Install the CPU version of PyTorch and torchvision\n",
    "        * A GPU is **not required** for this assigment. \n",
    "        * For more specific intructions related to your machine, please refer to: https://pytorch.org/get-started/locally/.\n",
    "    * Install scikit-learn **1.0.2**\n",
    "        * Note that in version 1.2.2, KNN classifier with L2 distance behaves weirdly. For example, testing training samples will cause an error. Query results vary a lot over different runs. Classification accurary is close to a random guess. We would recommend using version 1.0.2 instead. \n",
    "    * Install scikit-image, numpy, os, matplotlib, tqdm, torchinfo, etc.\n",
    "        * Use `pip install` to install any other required library.\n",
    "2. We have provided you with a number of functions and scripts in the hopes of alleviating some tedious or error-prone sections of the implementation. You are free to modify them if necessary. The provided files include:\n",
    "    * `dataset.py`\n",
    "        * Functions used to download, load and visualize a dataset\n",
    "    * `extract_feature.py`\n",
    "        * Functions to extract a variety of features, including raw pixels, HoG features and CNN features.\n",
    "    * `vgg_network.py`\n",
    "        * Class defining the VGG-11 architecture, functions to load a pretrained VGG model and test it. \n",
    "    * `path.py`\n",
    "        * Paths of directories to save the dataset, features, models, and figures. Feel free to reconfigure it.\n",
    "3. Submission\n",
    "    * <font color='magenta'><b>This notebook includes various things for you to do.</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>1. Load the data</font>\n",
    "\n",
    "### Download the data set\n",
    "We will be using the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) data set for our experiments. Before getting started, run the following block to download the data set. The data set will be saved to directory `/datasets`. You can change the default download path in `path.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import download_cifar10_dataset\n",
    "dataset = download_cifar10_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the data set\n",
    "\n",
    "Run the following block to load the training images, training labels, test images, and test labels as `x_train`, `y_train`, `x_test`, `y_test` respectively. The code will print out their shapes for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_dataset_splits\n",
    "x_train, y_train, x_test, y_test = load_dataset_splits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the data\n",
    "Run the following block to display several randomly-sampled images from each of the ten classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from dataset import visualize_cifar_data\n",
    "visualize_cifar_data(images=x_train.transpose(0, 2, 3, 1), labels=y_train, samples_per_class=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>2. Nearest neighbor classification on CIFAR-10</font>\n",
    "\n",
    "In this part, we will investigate the classification problem by training and testing a nearest neighbor classifier on CIFAR-10 dataset with **Euclidean (L2) distance**. \n",
    "\n",
    "Function `run_nearest_neighbor`, shown in the next cell, takes a labeled training set (`x_train`, `y_train`) and test set (`x_test`, `y_test`), and applies 1-nearest neighbor classification to the test points, using `KNeighborsClassifier` from `sklearn`. It prints out the accuracy on the test set and returns the classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def run_nearest_neighbor(x_train, y_train, x_test, y_test):\n",
    "    # create classifier\n",
    "    nn_classifier = KNeighborsClassifier(n_neighbors=1, algorithm='auto')\n",
    "    \n",
    "    # train \n",
    "    nn_classifier.fit(x_train, y_train)\n",
    "\n",
    "    # test and report accuracy\n",
    "    test_acc = nn_classifier.score(x_test, y_test)\n",
    "    print(\"Nearest neighbor accuracy on the test set: %f\"%test_acc)\n",
    "    \n",
    "    return nn_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest neighbor on raw pixels\n",
    "\n",
    "Now we do nearest neighbor classification in the raw pixel space. \n",
    "\n",
    "We have provided you with a function `compute_or_load_features` in `extract_feature.py` to extract the features from the training and test images. You can specify the feature extraction method by setting parameter `feature_type` to one of the options in `['raw_pixel', 'hog', 'pretrained_cnn', 'random_cnn']`. When the features are extracted by a CNN, you can further specify at which layer you are extracting the features by setting parameter `layer` to one of the options in `['last_conv', 'last_fc']`. In practice, extracting features from a large data set could be time consuming. To avoid repeated computation, this function will only compute the features once and store them in directory `/features` (you can change this in `path.py`). If being called later, the function will search for the existing feature files and directly load them into memory. \n",
    "\n",
    "Call function `compute_or_load_features` to extract the raw pixels as features and call function `run_knn` to train and test nearest neighbor classifier in this feature space. <font color='magenta'>To do: Report the test accuracy you get.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract_feature import compute_or_load_features\n",
    "\n",
    "# compute or load features\n",
    "raw_pixel_train_features, raw_pixel_test_features = compute_or_load_features(x_train, x_test, \"raw_pixel\")\n",
    "\n",
    "# run knn\n",
    "raw_pixel_knn_classifier = run_nearest_neighbor(raw_pixel_train_features, y_train, raw_pixel_test_features, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a sense of how the images are distributed locally in the pixel space, we can look at their nearest neighbors. \n",
    "\n",
    "<font color='magenta'>To do: Write code in the following block to do the following. Make your code modular so that it can be re-used for the representations we will consider.</font>\n",
    "* <font color='magenta'>Show the first five images in the test set whose label is correctly predicted by 1-NN, and show the nearest neighbor (in the training set) of each of these images.</font>\n",
    "* <font color='magenta'>Show the first five images in the test set whose label is incorrectly predicted by 1-NN, and show the nearest neighbor (in the training set) of each of the images.</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest neighbor on HOG features \n",
    "The HOG (Histogram of Oriented Gradients) descriptor computes local statistics of gradients in an image and uses them as feature respresentations. Train and test a nearest neighbor classifier on HOG features. <font color='magenta'>To do: Report the test accuracy. As with the raw pixel representation, show the first five correctly classified images (and their nearest neighbors) and the five first incorrectly classified.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract_feature import compute_or_load_features\n",
    "\n",
    "# compute or load features\n",
    "hog_train_features, hog_test_features = compute_or_load_features(x_train, x_test, \"hog\")\n",
    "    \n",
    "# run knn\n",
    "hog_knn_classifier = run_nearest_neighbor(hog_train_features, y_train, hog_test_features, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest neighbor on CNN features\n",
    "\n",
    "Over the past decade, deep <em>convolutional neural networks</em> (CNNs) have become building blocks in a wide range of computer vision tasks. A CNN trained on a large scale image classification task learns to extract spatial hierarchies of features from edges to object parts [2].  \n",
    "\n",
    "In this problem, we will explore the representations learned by CNNs on different layers. We will be using [VGG](https://arxiv.org/abs/1409.1556) as our convolutional network architecture. A VGG11 model pretrained on CIFAR-10 can be found at `/models/vgg11_bn.pt`, so you are **NOT REQUIRED** to train the neural network. And since we only need to do forward pass through the network (no backpropagation), a CPU is enough for our purpose.\n",
    "\n",
    "A VGG network is composed of a sequence of convolutional layers, pooling layers, and fully connected layers. To clearly understand its architecture, we provide you with a helper function visualizing the layers and input/output shapes. The following code feeds a batch of 32 × 32 × 3 images into VGG11 network and do a forward pass. Run the code and check the summary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "from vgg_network import vgg11_bn\n",
    "\n",
    "vgg_model = vgg11_bn(pretrained=False)\n",
    "summary(vgg_model, input_size=(16, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest neighbor on pretrained CNN features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the summary above, a VGG net consists of three components: \n",
    "1. A sequence of convolutional blocks (1-1 and 1-2)\n",
    "2. Two fully connected blocks (2-30 to 2-35)\n",
    "3. A single fully connected layer (2-36)\n",
    "\n",
    "The first two components together act as a feature extractor. The convolutional blocks extract the low-level and intermediate-level features, and the fully connected blocks extract the high-level features. The third component is a single linear layer mapping the feature vectors to the classes therefore can be viewed as a linear classifier. \n",
    "\n",
    "In this problem, we will experiment with features extracted by a pretrained VGG net at two specific layers:\n",
    "1. `last_conv`: AdaptiveAvgPool2d (1-2)\n",
    "2. `last_fc`: ReLU (2-34)\n",
    "\n",
    "Train and test a nearest neighbor classifier on pretrained VGG features at these two layers respectively (Set parameter `layer` of function `compute_or_load_features` to `last_conv` or `last_fc`). <font color='magenta'>Report the test accuracies with these two representations. For `last_fc`, show the first five correctly classified images (with nearest neighbors) and the first five incorrectly classified.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract_feature import compute_or_load_features\n",
    "\n",
    "# compute or load features\n",
    "pretrained_cnn_last_conv_train_features, pretrained_cnn_last_conv_test_features = compute_or_load_features(x_train, x_test, \"pretrained_cnn\", \"last_conv\")\n",
    "\n",
    "# run knn\n",
    "pretrained_cnn_last_conv_knn_classifier = run_nearest_neighbor(pretrained_cnn_last_conv_train_features, y_train, pretrained_cnn_last_conv_test_features, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract_feature import compute_or_load_features\n",
    "\n",
    "# compute or load features\n",
    "pretrained_cnn_last_fc_train_features, pretrained_cnn_last_fc_test_features = compute_or_load_features(x_train, x_test, \"pretrained_cnn\", \"last_fc\")\n",
    "\n",
    "# run knn\n",
    "pretrained_cnn_last_fc_knn_classifier = run_nearest_neighbor(pretrained_cnn_last_fc_train_features, y_train, pretrained_cnn_last_fc_test_features, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alignment between nearest neighbor classifier and pretrained CNN model\n",
    "In the following block, call function `test_pretrained_vgg` from `vgg_network.py`. The function will test the pretrained VGG net on the test images of CIFAR-10 and return the test accuracy. Report this accuracy and compare it with the test accuracy of nearest neighbor classifier in pretrained `last conv` and `last fc` feature space. In which space does the nearest neighbor classifier behave the most similarly to a pretrained CNN model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vgg_network import test_pretrained_vgg\n",
    "\n",
    "test_acc = test_pretrained_vgg(x_test, y_test)\n",
    "print('Accuracy on the test images: ', test_acc) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest neighbor on random CNN features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The excellent feature extraction ability of ConvNets enables them to solve computer vision problems in a universal way. One may think that this is because ConvNets are generally trained on large datasets of images. But is this the only reason?  To answer this question, a recent work Deep Image Prior [1] shows that the structure of a CNN is sufficient to capture a great deal of low-level image statistics prior to any learning. A randomly initialized fully-convolutional neural network is already able to achieve good results in standard image reconstruction problems such as denoising, super-resolution, and inpainting. \n",
    "\n",
    "In this problem, we will investigate whether the inductive bias captured by the ConvNets also helps for image classification problems.\n",
    "\n",
    "You will train and test the nearest neighbor classifier on the features extracted by a randomly initialized VGG network. <font color='magenta'>To do: Experiment with two feature spaces `last conv` and `last fc`, and report the test accuracies.</font>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract_feature import compute_or_load_features\n",
    "\n",
    "# compute or load features\n",
    "random_cnn_last_conv_train_features, random_cnn_last_conv_test_features = compute_or_load_features(x_train, x_test, \"random_cnn\", \"last_conv\")\n",
    "\n",
    "# run knn\n",
    "random_cnn_last_conv_knn_classifier = run_nearest_neighbor(random_cnn_last_conv_train_features, y_train, random_cnn_last_conv_test_features, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract_feature import compute_or_load_features\n",
    "\n",
    "# compute or load features\n",
    "random_cnn_last_fc_train_features, random_cnn_last_fc_test_features = compute_or_load_features(x_train, x_test, \"random_cnn\", \"last_fc\")\n",
    "\n",
    "# run knn\n",
    "random_cnn_last_fc_knn_classifier = run_nearest_neighbor(random_cnn_last_fc_train_features, y_train, random_cnn_last_fc_test_features, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the test accuracies of nearest neighbor classifier on random `last conv` features and HOG features. In which representation space does it perform better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the test accuracies of nearest neighbor classifier on random `last conv` features and random `last fc` features. In which representation space does it perform better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. D. Ulyanov, A. Vedaldi, and V. Lempitsky, [Deep Image Prior](https://arxiv.org/pdf/1711.10925.pdf), CVPR 2018.\n",
    "2. M. Zeiler and R. Fergus, [Visualizing and Understanding Deep Neural Networks](https://arxiv.org/pdf/1311.2901.pdf), ECCV 2014.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "279px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
